{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg19tfgpgUn8"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
        "!rm spark-3.4.0-bin-hadoop3.tgz   # Tidying up"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XaqJbRQ4XZk",
        "outputId": "0bf4813e-dbdb-48ab-89e8-8ff91910a9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-16 04:10:51--  https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
            "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
            "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 388407094 (370M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.4.0-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.4.0-bin-had 100%[===================>] 370.41M   268MB/s    in 1.4s    \n",
            "\n",
            "2023-05-16 04:10:52 (268 MB/s) - ‘spark-3.4.0-bin-hadoop3.tgz’ saved [388407094/388407094]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up our environmental variables: \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\""
      ],
      "metadata": {
        "id": "e61U8Ri64Xcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "pymNqNB74Xer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDReA-reij8E",
        "outputId": "0d2a158c-30f2-4c74-bf80-e4f9a0e7dc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/BIGDATA_assignment_2/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSPJxuSuiqh1",
        "outputId": "c0d23257-d849-48c5-cb04-72c5641267e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/BIGDATA_assignment_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps1lK2kDisEP",
        "outputId": "dad75157-146f-4195-ade5-23b58fa00ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "article-ids.txt\t\t    spark-3.3.1-bin-hadoop3\n",
            "combiner_bigram.py\t    spark-3.4.0-bin-hadoop3\n",
            "COMP47470_assignment_2.pdf  spark-3.4.0-bin-hadoop3.tgz.1\n",
            "lab4_config.sh\t\t    spark-3.4.0-bin-hadoop3.tgz.2\n",
            "mapper_bigram.py\t    stopwords.txt\n",
            "mapper_co-occur.py\t    tester_args_mapper.py\n",
            "nohup.out\t\t    tester_args_reduce.py\n",
            "parse_article.py\t    tester_hadoop.py\n",
            "parse_article.sh\t    wikipedia-ml\n",
            "reducer_bigram.py\t    wikipedia-ml-raw\n",
            "reducer_co-occur.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "ou3HnyH0izRs",
        "outputId": "ba7bd096-8bfe-4e27-a68c-c9216e18a6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f777066c430>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://653996b09d6c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "print(multiprocessing.cpu_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gi93xtgFi2QE",
        "outputId": "246af2dd-c3eb-4a5a-b621-9971b59fd44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_files = spark.sparkContext.wholeTextFiles(\"./wikipedia-ml/*.txt\")\n",
        "#test\n",
        "print(\"Number of Lines total: \", text_files.count())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1me5DJu4Xmf",
        "outputId": "839facac-007e-4edf-a519-da06cb7fe319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Lines total:  42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "count_word = text_files.filter(lambda x: \"Machine\" in x[1]).count()\n",
        "\n",
        "print(f\"Number of lines with 'Machine': {count_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ky_Q_01i4XpL",
        "outputId": "1dfd11ff-f152-45bd-d18c-fc1b332d8847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines with 'Machine': 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the file names from the keys and print them\n",
        "file_names = text_files.keys().collect()\n",
        "for name in file_names:\n",
        "    print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_safMIx4Xro",
        "outputId": "e6867bd1-ad6b-44e9-b527-f8c3f3835012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2013_1_2.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2013_6_27.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2014_1_4.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2014_6_4.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2015_1_4.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2015_6_30.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2016_1_1.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2016_6_30.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2017_1_1.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2017_6_29.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2018_1_5.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2018_6_29.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2019_1_2.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2019_6_24.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2020_1_8.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2020_6_30.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2021_1_8.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2021_6_19.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2022_1_2.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2022_6_28.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2023_1_1.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2013_1_2.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2013_6_27.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2014_1_4.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2014_6_4.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2015_1_4.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2015_6_30.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2016_1_1.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2016_6_30.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2017_1_1.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2017_6_29.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2018_1_5.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2018_6_29.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2019_1_2.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2019_6_24.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2020_1_8.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2020_6_30.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2021_1_8.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2021_6_19.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2022_1_2.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2022_6_28.txt\n",
            "file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/refs_Machine learning - Wikipedia_2023_1_1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stopwords = spark.sparkContext.textFile(\"./stopwords.txt\")\n",
        "\n",
        "#print(\"Number of words total: \", stopwords.count())"
      ],
      "metadata": {
        "id": "S0UMWHL34XuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_file = text_files.first()\n",
        "print(first_file[1][:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlC0okMQ4Xwz",
        "outputId": "7ad0c58f-be58-4efb-ca7d-10f7aa0c6d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an old revision of this page, as edited by 86.92.21.59 talk at 19:55, 2 January 2013. The pr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_files = text_files.take(2)\n",
        "for file in few_files:\n",
        "    print(\"File: \", file[0])\n",
        "    print(\"Content: \", file[1][:700])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA60YAPJ4XzQ",
        "outputId": "5c6bbe52-a882-4735-da6d-f0bace1840c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File:  file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2013_1_2.txt\n",
            "Content:  This is an old revision of this page, as edited by 86.92.21.59 talk at 19:55, 2 January 2013. The present address URL is a permanent link to this revision, which may differ significantly from the current revision.\n",
            "Machine learning, a branch of artificial intelligence, is about the construction and study of systems that can learn from data. For example, a machine learning system could be trained on email messages to learn to distinguish between spam and non-spam messages. After learning, it can then be used to classify new email messages into spam and non-spam folders. \n",
            "The core of machine learning deals with representation and generalization. Representation of data instances and functions ev\n",
            "File:  file:/content/drive/MyDrive/BIGDATA_assignment_2/wikipedia-ml/article_Machine learning - Wikipedia_2013_6_27.txt\n",
            "Content:  This is an old revision of this page, as edited by 115.248.253.41 talk at 09:35, 27 June 2013 Genetic programming. The present address URL is a permanent link to this revision, which may differ significantly from the current revision.\n",
            "Machine learning, a branch of artificial intelligence, is about the construction and study of systems that can learn from data. For example, a machine learning system could be trained on email messages to learn to distinguish between spam and non-spam messages. After learning, it can then be used to classify new email messages into spam and non-spam folders. \n",
            "The core of machine learning deals with representation and generalization. Representation of data insta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./stopwords.txt\", 'r') as f:\n",
        "    stopwords = f.read().split()\n",
        "\n",
        "print(stopwords[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnqka2hZY7Wb",
        "outputId": "5a6207aa-ecd2-4b6e-e6b1-fd6e295a4a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import string\n",
        "\n",
        "def rem_punc(arr):\n",
        "     arr = arr.translate(str.maketrans('', '', string.punctuation))\n",
        "     return arr\n",
        "\n",
        "def rem_sw(arr, stopwords):\n",
        "     new_arr=[]\n",
        "     for i in arr:\n",
        "          if i in stopwords:\n",
        "               pass\n",
        "          else:\n",
        "               new_arr.append(i)\n",
        "     return new_arr\n",
        "\n",
        "\n",
        "def map_cooccur(group, stopwords):\n",
        "    for g in group:\n",
        "        rg = rem_punc(g[1])\n",
        "        words = rg.lower().split()\n",
        "        words = rem_sw(words, stopwords)\n",
        "        pairs = []\n",
        "        for i in range(0, len(words)):\n",
        "            for j in range(i+1, len(words)):\n",
        "                pairs.append((words[i], words[j])) # tuple of words and count\n",
        "    # returned as key value pair with count of 1 for each\n",
        "    res = [(pair, 1) for pair in pairs]\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "LRS7Q4taI7J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reducer(count1, count2):\n",
        "    return count1 + count2"
      ],
      "metadata": {
        "id": "XEYCMqJRPZg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chat gpt function\n",
        "import re\n",
        "\n",
        "def extract_year(filename):\n",
        "    match = re.search(r'_(\\d{4})_', filename)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "years = text_files.groupBy(lambda x: extract_year(x[0]))"
      ],
      "metadata": {
        "id": "zuDP1nwZd9fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groups = years.collect()\n",
        "print(groups)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBcCgSS2GGQ5",
        "outputId": "4d3e7b31-874f-4f06-9c43-a656b2f29a47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('2013', <pyspark.resultiterable.ResultIterable object at 0x7f7778acd510>), ('2014', <pyspark.resultiterable.ResultIterable object at 0x7f7778acf5b0>), ('2015', <pyspark.resultiterable.ResultIterable object at 0x7f7778ace740>), ('2016', <pyspark.resultiterable.ResultIterable object at 0x7f7778acf280>), ('2017', <pyspark.resultiterable.ResultIterable object at 0x7f7778acf9d0>), ('2019', <pyspark.resultiterable.ResultIterable object at 0x7f7778acf0a0>), ('2020', <pyspark.resultiterable.ResultIterable object at 0x7f7778acf460>), ('2018', <pyspark.resultiterable.ResultIterable object at 0x7f777066d1e0>), ('2021', <pyspark.resultiterable.ResultIterable object at 0x7f777066c460>), ('2022', <pyspark.resultiterable.ResultIterable object at 0x7f777066dd80>), ('2023', <pyspark.resultiterable.ResultIterable object at 0x7f777066e530>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "years"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN6B-nCOStJM",
        "outputId": "531495e5-91fd-4f24-b4bd-86e19019a520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[11] at collect at <ipython-input-31-86814a6d380a>:1"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from functools import reduce\n",
        "\n",
        "group_results = years.map(lambda x: (x[0], map_cooccur(x[1], stopwords))) #.reduceByKey(lambda x, y: reduce(lambda a, b: a + b, [x, y]))\n",
        "\n",
        "flat_results = group_results.values().flatMap(lambda x: x).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# could not find a way to run the reduce properly and keep the year data so chose to move ahead with just the one RDD\n",
        "#final_results = flat_results.map(lambda x: ((extract_year(x[0]), x[0][0], x[0][1]), x[1])).groupByKey() "
      ],
      "metadata": {
        "id": "9h_vFto7MSeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arRN8ghEUvW2",
        "outputId": "35b4134e-4305-4b4d-e2e4-2b6420a0e105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[16] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flat_results.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCCi1OQUOxYy",
        "outputId": "2550a021-4149-4402-de51-98392e8ec99f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('wernick', 'yourganov'), 6),\n",
              " (('wernick', 'machine'), 77),\n",
              " (('wernick', 'learning'), 103),\n",
              " (('wernick', 'medical'), 6),\n",
              " (('wernick', '4'), 14),\n",
              " (('wernick', 'july'), 7),\n",
              " (('wernick', '2010'), 11),\n",
              " (('wernick', '2538'), 6),\n",
              " (('wernick', 'phil'), 3),\n",
              " (('wernick', 'simon'), 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, split\n",
        "\n",
        "schema_df = flat_results.map(lambda x: (x[0][0], x[0][1], x[1])).toDF([\"word_0\", \"word_1\", \"count\"])\n",
        "\n",
        "# sorting in advance\n",
        "schema_df = schema_df.orderBy(col(\"count\").desc()) \n"
      ],
      "metadata": {
        "id": "Rnzd-VAKI7UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema_df.select(\"*\").show(10)"
      ],
      "metadata": {
        "id": "9jirstpAI753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f34e9c5-f359-427a-d9df-c5f0c39b6bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-----+\n",
            "|   word_0|   word_1|count|\n",
            "+---------+---------+-----+\n",
            "| learning|retrieved| 9436|\n",
            "| learning| learning| 9381|\n",
            "| learning|  machine| 7444|\n",
            "|  machine| learning| 7199|\n",
            "|  machine|retrieved| 6836|\n",
            "|  machine|  machine| 5449|\n",
            "|retrieved|retrieved| 4575|\n",
            "|     isbn| learning| 4183|\n",
            "|     isbn|retrieved| 3816|\n",
            "|retrieved| learning| 3594|\n",
            "+---------+---------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part2"
      ],
      "metadata": {
        "id": "rEku9qOWkf9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string \n",
        "\n",
        "import itertools\n",
        "import string\n",
        "\n",
        "def rem_punc(arr):\n",
        "     arr = arr.translate(str.maketrans('', '', string.punctuation))\n",
        "     return arr\n",
        "\n",
        "def map_bigram(group):\n",
        "    for g in group:\n",
        "        rg = rem_punc(g[1])\n",
        "        rg = rg.strip()\n",
        "        words = rg.lower().split()\n",
        "        pairs = []\n",
        "        for i in range(2, len(words)):\n",
        "            pairs.append((words[i-2], words[i-1], words[i]))\n",
        "        res = [(pair, 1) for pair in pairs]\n",
        "        return res"
      ],
      "metadata": {
        "id": "ABSlpvo-I8Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "group_results = years.map(lambda x: (x[0], map_bigram(x[1]))) #.reduceByKey(lambda x, y: reduce(lambda a, b: a + b, [x, y]))\n",
        "\n",
        "flat_results = group_results.values().flatMap(lambda x: x).reduceByKey(lambda x, y: x + y)"
      ],
      "metadata": {
        "id": "KS0cwWSFkbvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flat_results.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijp52C1Akb9V",
        "outputId": "e6191dcd-25ac-44bd-f54a-e2bc7659c9f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('is', 'an', 'old'), 11),\n",
              " (('an', 'old', 'revision'), 11),\n",
              " (('old', 'revision', 'of'), 11),\n",
              " (('as', 'edited', 'by'), 11),\n",
              " (('edited', 'by', '86922159'), 1),\n",
              " (('86922159', 'talk', 'at'), 1),\n",
              " (('2', 'january', '2013'), 1),\n",
              " (('january', '2013', 'the'), 1),\n",
              " (('the', 'present', 'address'), 11),\n",
              " (('address', 'url', 'is'), 11)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema_df2 = flat_results.map(lambda x: (x[0][0], x[0][1], x[0][2], x[1])).toDF([\"word_0\", \"word_1\", \"word_1\", \"count\"])\n",
        "\n",
        "# sorting in advance\n",
        "schema_df2 = schema_df2.orderBy(col(\"count\").desc()) "
      ],
      "metadata": {
        "id": "Ljap5N-8kcKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema_df2.select(\"*\").show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85XfY4OSkcYL",
        "outputId": "e1362e54-fa97-4bf7-87aa-a95ca4c694f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+-----+\n",
            "| word_0|    word_1|    word_1|count|\n",
            "+-------+----------+----------+-----+\n",
            "|     of|   machine|  learning|  104|\n",
            "|      a|       set|        of|  102|\n",
            "|machine|  learning|algorithms|   65|\n",
            "|machine|  learning|        is|   61|\n",
            "| sparse|dictionary|  learning|   42|\n",
            "|     of|         a|       set|   38|\n",
            "|     be|      used|        to|   37|\n",
            "|      a|   machine|  learning|   37|\n",
            "|     in|   machine|  learning|   36|\n",
            "|    the|     field|        of|   35|\n",
            "+-------+----------+----------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ATbeK5f-kclb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fDKOai0Hkcy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDrpeIkekdAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zePZu4RykdON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GV0m79u0kddw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3uGUmMPI8Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "URbbepxlI8Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVT_SRPGI8Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boHkbjGL4X7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}